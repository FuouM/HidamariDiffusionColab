{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuouM/HidamariDiffusionColab/blob/main/Neo_Hidamari_Diffusion_wip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtJ1grxqfG8u"
      },
      "source": [
        "Code taken from https://colab.research.google.com/drive/1yf3-bUhTcfxRmAphJHVQQAD2ArYO1CRZ\n",
        "\n",
        "Guides:\n",
        "\n",
        "https://rentry.org/retardsguide - Main txt2img guide\n",
        "https://rentry.org/kretard - K-Diffusion guide\n",
        "https://rentry.org/img2img - img2img guide\n",
        "\n",
        "Updated to use the new model with old code\n",
        "\n",
        "Recommend using hlky's fork"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dTlNpVz7d_v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwZ0GT0eObBW"
      },
      "source": [
        "### Download and set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTjD9Ij7Nuh4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/basujindal/stable-diffusion\n",
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cdn.discordapp.com/attachments/1011261946223394877/1012027527792951377/NotoSansJP-Bold.otf"
      ],
      "metadata": {
        "id": "HtnJTmg8SIrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pxSsF1HOi4V"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install albumentations==0.4.3\n",
        "!pip install opencv-python==4.1.2.30\n",
        "!pip install pudb==2019.2\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.2\n",
        "#!pip install pytorch-lightning==1.4.2\n",
        "!pip install pytorch-lightning \n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install test-tube>=0.7.5\n",
        "!pip install streamlit>=0.73.1\n",
        "!pip install einops==0.3.0\n",
        "!pip install torch-fidelity==0.3.0\n",
        "# !pip install pilmoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQILJaTuZCLU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52gTy8xUZD_5"
      },
      "outputs": [],
      "source": [
        "!yes w | pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!yes w | pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git src/k-diffusion\n",
        "!pip install src/k-diffusion\n",
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPIqZIAu_SNj"
      },
      "outputs": [],
      "source": [
        "!mkdir -p '/content/stable-diffusion/Source'\n",
        "!mkdir -p '/content/stable-diffusion/Output'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnULtFtROkqr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # This will crash Colab (required, everything will still be intact so dont worry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHTXI7KOnu4"
      },
      "source": [
        "### Download the model (Choose only 1 option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAw5HFJRO4b-"
      },
      "outputs": [],
      "source": [
        "!gdown 1athuOO6kKtvLm3x1zqxNN8fqBIzV15ss -O /content/stable-diffusion/model.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1CqjnK1Ik0J"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion/\n",
        "!wget -O model.ckpt \"https://drive.yerf.org/wl/?id=EBfTrmcCCUAGaQBXVIj5lJmEhjoP1tgl&mode=grid&download=1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Gdrive"
      ],
      "metadata": {
        "id": "yaHfjOHyJgdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "s1CMRG1FJo2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_path = \"\" #@param {type: 'string'}\n",
        "file_name = \"sd-v1-4.ckpt\" #@param {type: 'string'}\n",
        "%cd {user_path}\n",
        "\n",
        "!cp {file_name} \"/content/stable-diffusion/\"\n",
        "\n",
        "%cd /content/stable-diffusion\n"
      ],
      "metadata": {
        "id": "rb-O9TBEJt2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename your model file (if it isn't model.ckpt)"
      ],
      "metadata": {
        "id": "Gbo4OrWjcAsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv {file_name} model.ckpt"
      ],
      "metadata": {
        "id": "4ZJ_5_8zb8EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Huggingface"
      ],
      "metadata": {
        "id": "N-ObhCcRHhFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_token = \"\" #@param {type:\"string\"}\n",
        "user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "!wget --header={user_header} https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt -O /content/stable-diffusion/model.ckpt"
      ],
      "metadata": {
        "id": "3BQoPx_8Hj08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimized SD"
      ],
      "metadata": {
        "id": "Oudkt8AEKw05"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmQfJWm8QGoA"
      },
      "source": [
        "## Text 2 Image (Optimized SD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7syo80FPa4T"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY4lXPAhbU88"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgwY7MOYXfja"
      },
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob, random\n",
        "import torch\n",
        "import numpy as np\n",
        "import copy\n",
        "from random import randint\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from ldm.util import instantiate_from_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvnfjJvQXjiY"
      },
      "outputs": [],
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw \n",
        "\n",
        "def add_margin(pil_img, top, right, bottom, left, color):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def text_wrap(text, font, max_width):\n",
        "        lines = []\n",
        "        if font.getsize(text)[0]  <= max_width:\n",
        "            lines.append(text)\n",
        "        else:\n",
        "            words = text.split(' ')\n",
        "            i = 0\n",
        "            while i < len(words):\n",
        "                line = ''\n",
        "                while i < len(words) and font.getsize(line + words[i])[0] <= max_width:\n",
        "                    line = line + words[i]+ \" \"\n",
        "                    i += 1\n",
        "                if not line:\n",
        "                    line = words[i]\n",
        "                    i += 1\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "def caption(image, prompt, info):\n",
        "  width, height = image.size\n",
        "\n",
        "  font = ImageFont.truetype(\"/content/stable-diffusion/NotoSansJP-Bold.otf\", 20, encoding='utf-8')\n",
        "  lines = text_wrap(prompt, font, image.size[0])\n",
        "  lines.append(f\"{info}\")\n",
        "  line_height = font.getsize('hg')[1]\n",
        "  cap_img = add_margin(image, 0, 0, line_height * (len(lines) + 1), 0, (255, 255, 255))\n",
        "  draw = ImageDraw.Draw(cap_img)\n",
        "  pad = 2\n",
        "  x = pad * 2\n",
        "  y = height + pad\n",
        "  for line in lines:\n",
        "      draw.text((x,y), line, fill=(0, 0, 0), font=font)\n",
        "      y = y + line_height\n",
        "  return cap_img"
      ],
      "metadata": {
        "id": "H0LAycy3dbul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihQD4BD1P1Vr"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcSFxhq4P8kl"
      },
      "outputs": [],
      "source": [
        "config = \"optimizedSD/v1-inference.yaml\"\n",
        "ckpt = \"model.ckpt\"\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li, lo = [], []\n",
        "for key, v_ in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)\n",
        "\n",
        "config = OmegaConf.load(f\"{config}\")\n",
        "config.modelUNet.params.small_batch = False\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "del sd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, ddim_steps, batch_size, Height, Width, seed, n_rows, scale):\n",
        "   \n",
        "    full_precision = False\n",
        "    small_batch = False\n",
        "    device = \"cuda\"\n",
        "    C = 4\n",
        "    f = 8\n",
        "    start_code = None\n",
        "    ddim_eta = 0.0\n",
        "    n_iter = 1\n",
        "    \n",
        "    model.small_batch = small_batch\n",
        "\n",
        "    if not full_precision:\n",
        "        model.half()\n",
        "        modelCS.half()\n",
        "    \n",
        "    if seed == '':\n",
        "        seed = randint(0, 1000000)\n",
        "    seed = int(seed)\n",
        "    print(\"init_seed = \", seed)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    precision_scope = autocast if not full_precision else nullcontext\n",
        "\n",
        "    all_samples = []\n",
        "    output_images = list()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        all_samples = list()\n",
        "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    \n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    shape = [C, Height // f, Width // f]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "\n",
        "                    samples_ddim = model.sample(S=ddim_steps,\n",
        "                                    conditioning=c,\n",
        "                                    batch_size=batch_size,\n",
        "                                    seed = seed,\n",
        "                                    shape=shape,\n",
        "                                    verbose=False,\n",
        "                                    unconditional_guidance_scale=scale,\n",
        "                                    unconditional_conditioning=uc,\n",
        "                                    eta=ddim_eta,\n",
        "                                    x_T=start_code)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "\n",
        "                    for i in range(batch_size):\n",
        "                        \n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        all_samples.append(x_sample.to(\"cpu\"))\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        out = Image.fromarray(x_sample.astype(np.uint8))\n",
        "                        output_images.append(out)\n",
        "                        seed+=1\n",
        "\n",
        "\n",
        "\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "                    del samples_ddim\n",
        "                    del x_sample\n",
        "                    del x_samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "\n",
        "\n",
        "    grid = torch.cat(all_samples, 0)\n",
        "    grid = make_grid(grid, nrow=n_rows)\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    output_images.insert(0, Image.fromarray(grid.astype(np.uint8)))\n",
        "\n",
        "    return output_images"
      ],
      "metadata": {
        "id": "l6UxQ6dGcS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95iTXcZQbnk"
      },
      "source": [
        "### Run Text 2 Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmNXWU20QesI"
      },
      "outputs": [],
      "source": [
        "prompt = \"A blue corgi. At the table. Drinking tea. Next to the window. Dusk. Sun shines through the window. Light reflection on table.\" #@param {type:\"string\"}\n",
        "ddim_steps = 7 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "scale = 7 #@param {type:'integer'}\n",
        "batch_size = 4 #@param {type:'integer'}\n",
        "Height = 512 #@param {type:'integer'}\n",
        "Width = 512 #@param {type:'integer'}\n",
        "seed = 3213414 #@param {type:'integer'}\n",
        "n_rows = 2 #@param {type:'integer'}\n",
        "images = generate(prompt,ddim_steps, batch_size, Height, Width, seed, n_rows, scale)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeaR0VMfr_im"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / Funny errors (Run from Set up again)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {cfg_scale}, steps = {n_steps}, sampler = {sampler}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "JCrLSAUHddq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IAzahTMZVIE"
      },
      "outputs": [],
      "source": [
        "images[0] # First one is grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDQmNv_9ZTGc"
      },
      "source": [
        "#### Save images individually "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8AwaB3XZhfb"
      },
      "outputs": [],
      "source": [
        "images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWys2AGRZhVg"
      },
      "outputs": [],
      "source": [
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e72kpFjNZhL8"
      },
      "outputs": [],
      "source": [
        "images[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2D4kZNNJ_Uv"
      },
      "outputs": [],
      "source": [
        "images[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jepvv8Q-1a"
      },
      "source": [
        "## Image 2 Image (Optimized SD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdeC2LZRBm-"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stable-diffusion"
      ],
      "metadata": {
        "id": "Exl7IBQKc8bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw-VyYfuRBPn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import randint\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from einops import rearrange, repeat\n",
        "from contextlib import nullcontext\n",
        "from ldm.util import instantiate_from_config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "def load_img(image, h0, w0):\n",
        "   \n",
        "    image = Image.fromarray(image)\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h})\")   \n",
        "    if(h0 is not None and w0 is not None):\n",
        "        h, w = h0, w0\n",
        "    \n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
        "\n",
        "    print(f\"New image size ({w}, {h})\")\n",
        "    image = image.resize((w, h), resample = Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1."
      ],
      "metadata": {
        "id": "mPp1wgnoc_Vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw \n",
        "\n",
        "def add_margin(pil_img, top, right, bottom, left, color):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def text_wrap(text, font, max_width):\n",
        "        lines = []\n",
        "        if font.getsize(text)[0]  <= max_width:\n",
        "            lines.append(text)\n",
        "        else:\n",
        "            words = text.split(' ')\n",
        "            i = 0\n",
        "            while i < len(words):\n",
        "                line = ''\n",
        "                while i < len(words) and font.getsize(line + words[i])[0] <= max_width:\n",
        "                    line = line + words[i]+ \" \"\n",
        "                    i += 1\n",
        "                if not line:\n",
        "                    line = words[i]\n",
        "                    i += 1\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "def caption(image, prompt, info):\n",
        "  width, height = image.size\n",
        "\n",
        "  font = ImageFont.truetype(\"/content/stable-diffusion/NotoSansJP-Bold.otf\", 20, encoding='utf-8')\n",
        "  lines = text_wrap(prompt, font, image.size[0])\n",
        "  lines.append(f\"{info}\")\n",
        "  line_height = font.getsize('hg')[1]\n",
        "  cap_img = add_margin(image, 0, 0, line_height * (len(lines) + 1), 0, (255, 255, 255))\n",
        "  draw = ImageDraw.Draw(cap_img)\n",
        "  pad = 2\n",
        "  x = pad * 2\n",
        "  y = height + pad\n",
        "  for line in lines:\n",
        "      draw.text((x,y), line, fill=(0, 0, 0), font=font)\n",
        "      y = y + line_height\n",
        "  return cap_img"
      ],
      "metadata": {
        "id": "2YY_5NWndZ_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oPo2rboRWfH"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pAF_LzRZSr"
      },
      "outputs": [],
      "source": [
        "config = \"optimizedSD/v1-inference.yaml\"\n",
        "ckpt = \"model.ckpt\"\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li, lo = [], []\n",
        "for key, v_ in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)\n",
        "\n",
        "config = OmegaConf.load(f\"{config}\")\n",
        "config.modelUNet.params.small_batch = False\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "del sd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(image, prompt, strength, scale, ddim_steps, batch_size, Height, Width, seed, n_rows):\n",
        "   \n",
        "    device = \"cuda\"\n",
        "    scale = 7.5\n",
        "    n_iter = 1\n",
        "    small_batch = False\n",
        "    full_precision = False\n",
        "\n",
        "    model.small_batch = small_batch\n",
        "    \n",
        "    init_image = load_img(image, Height, Width).to(device)\n",
        "    if not full_precision:\n",
        "        model.half()\n",
        "        modelCS.half()\n",
        "        modelFS.half()\n",
        "        init_image = init_image.half()\n",
        "\n",
        "    \n",
        "    if seed == '':\n",
        "        seed = randint(0, 1000000)\n",
        "    seed = int(seed)\n",
        "    print(\"init_seed = \", seed)\n",
        "    seed_everything(seed)\n",
        "\n",
        "    # n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    modelFS.to(device)\n",
        "\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = modelFS.get_first_stage_encoding(modelFS.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    mem = torch.cuda.memory_allocated()/1e6\n",
        "    modelFS.to(\"cpu\")\n",
        "    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "    assert 0. <= strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(strength *ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if not full_precision else nullcontext\n",
        "\n",
        "    all_samples = []\n",
        "    output_images = list()\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        for _ in trange(n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    \n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    # encode (scaled latent)\n",
        "                    z_enc = model.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device), seed,ddim_steps)\n",
        "                    # decode it\n",
        "                    samples_ddim = model.decode(z_enc, c, t_enc, unconditional_guidance_scale=scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    print(\"saving images\")\n",
        "                    for i in range(batch_size):\n",
        "                        \n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        all_samples.append(x_sample.to(\"cpu\"))\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "\n",
        "                        output_images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "\n",
        "                        seed+=1\n",
        "\n",
        "\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "                    del samples_ddim\n",
        "                    del x_sample\n",
        "                    del x_samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "\n",
        "\n",
        "    grid = torch.cat(all_samples, 0)\n",
        "    grid = make_grid(grid, nrow=n_rows)\n",
        "    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "    output_images.insert(0, Image.fromarray(grid.astype(np.uint8)))\n",
        "\n",
        "    return output_images\n"
      ],
      "metadata": {
        "id": "vdfIbOnkdDMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMj_JbQ9Rrxq"
      },
      "source": [
        "### Run Image 2 Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdR7s1yQR3m4"
      },
      "outputs": [],
      "source": [
        "%cd '/content/stable-diffusion/Source'\n",
        "link = \"https://cdn.discordapp.com/attachments/692612547345514549/1010734092687519744/take_00474.png\" #@param {type:\"string\"}\n",
        "!wget {link}\n",
        "%cd  '/content/stable-diffusion/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q1Qeg3_RwKU"
      },
      "outputs": [],
      "source": [
        "prompt = \"a male hand holding a gun shooting a bullet with gun powder and explosion, hyperreal, artstation, masterpiece, oil painting, close shot\" #@param {type:\"string\"}\n",
        "img = \"/content/stable-diffusion/Source/take_00474.png\" #@param {type:\"string\"}\n",
        "init = np.asarray(Image.open(img))\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 384 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.99 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "seed = 204363062 #@param {type:\"integer\"};\n",
        "\n",
        "images = generate(init, prompt, strength, scale, n_steps, n_images, height, width, seed, n_rows)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / Funny errors (Run from Set up again)"
      ],
      "metadata": {
        "id": "L-NXHyjcdmRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {cfg_scale}, steps = {n_steps}, sampler = {sampler}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "bB674hn6dQ7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3zuCpuYNJ6_"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Individual outputs"
      ],
      "metadata": {
        "id": "iIBkjkG_dHAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgSuZn4DOKAs"
      },
      "outputs": [],
      "source": [
        "images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rHaSK3VP6ja"
      },
      "outputs": [],
      "source": [
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldZcKISoP8E5"
      },
      "outputs": [],
      "source": [
        "images[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ1-eVNhP8-k"
      },
      "outputs": [],
      "source": [
        "images[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kQ2bDBrPe6Q"
      },
      "source": [
        "### Running image sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DtBv6OVPiR_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import clear_output \n",
        "import numpy as np\n",
        "import timeit\n",
        "#@title Image sequence\n",
        "#@markdown ##### Enter text prompt:\n",
        "prompt = \"man standing in front of a painting of a mountain in the distance, wide angle, Van Gogh, futuristic, surreal, holy\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ##### Set sequence duration (/Source/prefix_XXXXX.png):\n",
        "start_frame = \"00474\" #@param {type:\"string\"}\n",
        "end_frame = \"00481\" #@param {type:\"string\"}\n",
        "prefix = \"take\" #@param {type:\"string\"}\n",
        "current_frame = start_frame\n",
        "#@markdown ---\n",
        "#@markdown ##### SD settings:\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 384 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 1 #@param {type:\"integer\"}\n",
        "n_rows = 1 #@param {type:\"integer\"}\n",
        "\n",
        "seed = 204363062 #@param {type:\"integer\"};\n",
        "timeStart = timeit.default_timer()\n",
        "for i in range(int(start_frame), int(end_frame) + 1):\n",
        "  current_frame = \"{:05d}\".format(i)\n",
        "  print(\"PROCESSING \" + current_frame)\n",
        "  img = \"/content/stable-diffusion/Source/{}_{}.png\".format(prefix, current_frame)\n",
        "  init = np.asarray(Image.open(img))\n",
        "  images = generate(init, prompt, strength, scale, n_steps, n_images, height, width, seed, n_rows)\n",
        "  \n",
        "  images[0].save(\"/content/stable-diffusion/Output/{}_{}.png\".format(prefix, current_frame))\n",
        "  images[0]\n",
        "\n",
        "  clear_output()\n",
        "timeStop = timeit.default_timer()\n",
        "print(\"{} frames done in {} seconds.\\nPrompt: {}\\nStrength: {}\".format(int(end_frame) - int(start_frame), timeStop - timeStart, prompt, strength))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1cRK744Ch4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / Funny errors (Run from Set up again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFnTkna0dP1K"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "!zip -r \"/content/stable-diffusion/result.zip\" \"/content/stable-diffusion/Output\"\n",
        "files.download('/content/stable-diffusion/result.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBmrTroGjzxt"
      },
      "outputs": [],
      "source": [
        "!rm \"/content/stable-diffusion/result.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtcgOzRhll5"
      },
      "outputs": [],
      "source": [
        "#@title Export video\n",
        "from google.colab import files\n",
        "file_name = \"out.mp4\" #@param {type: \"string\"}\n",
        "always_overwrite = True #@param {type:\"boolean\"}\n",
        "framerate = 30 #@param (type:number)\n",
        "if always_overwrite:\n",
        "  replace = 'y'\n",
        "else:\n",
        "  replace = 'n'\n",
        "!yes {replace} | ffmpeg -framerate {framerate} -start_number {int(start_frame)} -i /content/stable-diffusion/Output/{prefix}_%05d.png -c:v libx264 -crf 0 {file_name}\n",
        "files.download('/content/stable-diffusion/{}'.format(file_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fViEXufPjam6"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/stable-diffusion/Source/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np75eoywjbn8"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/stable-diffusion/Output/*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hlky fork (no GFPGAN) - Gradio-less\n",
        "\n",
        "Has K-diffusion but uses more VRAM\n",
        "\n",
        "https://github.com/hlky/stable-diffusion"
      ],
      "metadata": {
        "id": "YbgZNNTxLOEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ],
      "metadata": {
        "id": "fVPP3LONLmdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/stable-diffusion\n",
        "\n",
        "import argparse, os, sys, glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from torch import autocast\n",
        "\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "import k_diffusion as K\n",
        "import torch.nn as nn\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "ZcXB2r-TLmSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_img_pil(img_pil):\n",
        "    image = img_pil.convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h})\")\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    print(f\"cropped image to size ({w}, {h})\")\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2. * image - 1.\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    return load_img_pil(Image.open(path))\n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class KDiffusionSampler:\n",
        "    def __init__(self, m):\n",
        "        self.model = m\n",
        "        self.model_wrap = K.external.CompVisDenoiser(m)\n",
        "\n",
        "    def sample(self, S, conditioning, batch_size, shape, verbose, unconditional_guidance_scale, unconditional_conditioning, eta, x_T):\n",
        "        sigmas = self.model_wrap.get_sigmas(S)\n",
        "        x = x_T * sigmas[0]\n",
        "        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
        "        samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args={'cond': conditioning, 'uncond': unconditional_conditioning, 'cond_scale': unconditional_guidance_scale}, disable=False)\n",
        "\n",
        "        return samples_ddim, None\n",
        "\n",
        "\n",
        "def create_random_tensors(seed, shape, count, same_seed=False):\n",
        "    xs = []\n",
        "    for i in range(count):\n",
        "        current_seed = seed if same_seed else seed + i\n",
        "        torch.manual_seed(current_seed)\n",
        "        xs.append(torch.randn(shape, device=device))\n",
        "    x = torch.stack(xs)\n",
        "    return x\n",
        "\n",
        "def image_grid(imgs, batch_size, n_rows:int, round_down=False):\n",
        "    if n_rows > 0:\n",
        "        rows = n_rows\n",
        "    elif n_rows == 0:\n",
        "        rows = batch_size\n",
        "    else:\n",
        "        rows = math.sqrt(len(imgs))\n",
        "        rows = int(rows) if round_down else round(rows)\n",
        "\n",
        "    cols = math.ceil(len(imgs) / rows)\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols * w, rows * h), color='black')\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "\n",
        "    return grid"
      ],
      "metadata": {
        "id": "3I78LM7iLX4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updates"
      ],
      "metadata": {
        "id": "gkLTqrwyRcAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## UPDATE\n",
        "\n",
        "LANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n",
        "invalid_filename_chars = '<>:\"/\\|?*\\n'\n",
        "\n",
        "def resize_image(resize_mode, im, width, height):\n",
        "    if resize_mode == 0:\n",
        "        res = im.resize((width, height), resample=LANCZOS)\n",
        "    elif resize_mode == 1:\n",
        "        ratio = width / height\n",
        "        src_ratio = im.width / im.height\n",
        "\n",
        "        src_w = width if ratio > src_ratio else im.width * height // im.height\n",
        "        src_h = height if ratio <= src_ratio else im.height * width // im.width\n",
        "\n",
        "        resized = im.resize((src_w, src_h), resample=LANCZOS)\n",
        "        res = Image.new(\"RGB\", (width, height))\n",
        "        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n",
        "    else:\n",
        "        ratio = width / height\n",
        "        src_ratio = im.width / im.height\n",
        "\n",
        "        src_w = width if ratio < src_ratio else im.width * height // im.height\n",
        "        src_h = height if ratio >= src_ratio else im.height * width // im.width\n",
        "\n",
        "        resized = im.resize((src_w, src_h), resample=LANCZOS)\n",
        "        res = Image.new(\"RGB\", (width, height))\n",
        "        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n",
        "\n",
        "        if ratio < src_ratio:\n",
        "            fill_height = height // 2 - src_h // 2\n",
        "            res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n",
        "            res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\n",
        "        else:\n",
        "            fill_width = width // 2 - src_w // 2\n",
        "            res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n",
        "            res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "e3PYezWfOUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw \n",
        "\n",
        "def add_margin(pil_img, top, right, bottom, left, color):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def text_wrap(text, font, max_width):\n",
        "        lines = []\n",
        "        if font.getsize(text)[0]  <= max_width:\n",
        "            lines.append(text)\n",
        "        else:\n",
        "            words = text.split(' ')\n",
        "            i = 0\n",
        "            while i < len(words):\n",
        "                line = ''\n",
        "                while i < len(words) and font.getsize(line + words[i])[0] <= max_width:\n",
        "                    line = line + words[i]+ \" \"\n",
        "                    i += 1\n",
        "                if not line:\n",
        "                    line = words[i]\n",
        "                    i += 1\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "def caption(image, prompt, info):\n",
        "  width, height = image.size\n",
        "\n",
        "  font = ImageFont.truetype(\"/content/stable-diffusion/NotoSansJP-Bold.otf\", 20, encoding='utf-8')\n",
        "  lines = text_wrap(prompt, font, image.size[0])\n",
        "  lines.append(f\"{info}\")\n",
        "  line_height = font.getsize('hg')[1]\n",
        "  cap_img = add_margin(image, 0, 0, line_height * (len(lines) + 1), 0, (255, 255, 255))\n",
        "  draw = ImageDraw.Draw(cap_img)\n",
        "  pad = 2\n",
        "  x = pad * 2\n",
        "  y = height + pad\n",
        "  for line in lines:\n",
        "      draw.text((x,y), line, fill=(0, 0, 0), font=font)\n",
        "      y = y + line_height\n",
        "  return cap_img"
      ],
      "metadata": {
        "id": "vqRs04yoRett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "na17SiCBMY9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    ckpt = '/content/stable-diffusion/model.ckpt'\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/stable-diffusion/v1-inference.yaml\")\n",
        "model = load_model_from_config(config, \"models/ldm/stable-diffusion-v1/model.ckpt\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.half().to(device)"
      ],
      "metadata": {
        "id": "C8J52y2xMcti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_C = 4\n",
        "opt_f = 8"
      ],
      "metadata": {
        "id": "xUYLO7_BKnth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class info:\n",
        "  prompt = \"lmao\"\n",
        "  seed = 1234\n",
        "\n",
        "  sampler = KDiffusionSampler(model)\n",
        "  ddim_steps = 50\n",
        "  cfg_scale = 7\n",
        "  strength = 0.5\n",
        "\n",
        "  n_samples = 4\n",
        "  n_rows = 2\n",
        "\n",
        "\n",
        "  height = 512\n",
        "  width = 512\n",
        "\n",
        "  skip_grid = False\n",
        "  n_iter = 1\n",
        "  resize_mode = 0"
      ],
      "metadata": {
        "id": "FKhHacJARyuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = info()\n"
      ],
      "metadata": {
        "id": "B8cCs2iMLfWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(user_input)"
      ],
      "metadata": {
        "id": "v3oQvowaSF7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(user: info, mode: str, init_image):\n",
        "  assert user.prompt is not None\n",
        "  \n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  if user.seed == -1:\n",
        "    seed = random.randrange(4294967294)\n",
        "\n",
        "  batch_size = user.n_samples\n",
        "  prompts = batch_size * [prompt]\n",
        "\n",
        "  output_images = []\n",
        "  precision_scope = autocast\n",
        "  flag_img2img = False\n",
        "  if mode == \"img2img\":\n",
        "    flag_img2img = True\n",
        "    \n",
        "  with torch.no_grad(), precision_scope(\"cuda\"), model.ema_scope():\n",
        "    if flag_img2img:\n",
        "      \n",
        "      # model_wrap = K.external.CompVisDenoiser(model)\n",
        "\n",
        "      image = init_image.convert(\"RGB\")\n",
        "      image.save(\"/content/stable-diffusion/Output/please.png\")\n",
        "      image = resize_image(user.resize_mode, image, width, height)\n",
        "      # image = image.resize((width, height), resample=PIL.Image.Resampling.LANCZOS)\n",
        "      image = np.array(image).astype(np.float32) / 255.0\n",
        "      image = image[None].transpose(0, 3, 1, 2)\n",
        "      image = torch.from_numpy(image)\n",
        "      \n",
        "\n",
        "      init_image = 2. * image - 1.\n",
        "      init_image = init_image.to(device)\n",
        "      init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "      init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "      x0 = init_latent\n",
        "\n",
        "      assert 0. <= user.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "      t_enc = int(user.strength * user.ddim_steps)\n",
        "\n",
        "      sigmas = user.sampler.model_wrap.get_sigmas(user.ddim_steps)\n",
        "\n",
        "    \n",
        "    shape = [opt_C, height // opt_f, width // opt_f]\n",
        "\n",
        "\n",
        "    for n in range(user.n_iter):\n",
        "      uc = None\n",
        "      if user.cfg_scale != 1.0:\n",
        "        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "      if isinstance(prompts, tuple):\n",
        "        prompts = list(prompts)\n",
        "      c = model.get_learned_conditioning(prompts)\n",
        "      batch_seed = user.seed + n * batch_size\n",
        "      noise = create_random_tensors(batch_seed, shape, count=batch_size, same_seed=False)\n",
        "      if flag_img2img:\n",
        "        noise = noise * sigmas[user.ddim_steps - t_enc - 1]\n",
        "        xi = x0 + noise\n",
        "        sigma_sched = sigmas[user.ddim_steps - t_enc - 1:]\n",
        "        model_wrap_cfg = CFGDenoiser(user.sampler.model_wrap)\n",
        "        extra_args = {'cond': c, 'uncond': uc, 'cond_scale': user.cfg_scale}\n",
        "\n",
        "        samples_ddim = K.sampling.sample_lms(model_wrap_cfg, xi, sigma_sched, extra_args=extra_args, disable=False)\n",
        "      else: \n",
        "        samples_ddim, _ = user.sampler.sample(S=user.ddim_steps, conditioning=c, batch_size=len(prompts), shape=shape, verbose=False, unconditional_guidance_scale=user.cfg_scale, unconditional_conditioning=uc, eta=0.0, x_T=noise)\n",
        "\n",
        "      x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "      x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "      for i, x_sample in enumerate(x_samples_ddim):\n",
        "        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "        x_sample = x_sample.astype(np.uint8)\n",
        "\n",
        "        image = Image.fromarray(x_sample)\n",
        "          \n",
        "\n",
        "        output_images.append(image)\n",
        "        \n",
        "    if not user.skip_grid:\n",
        "      # additionally, save as grid\n",
        "      grid = image_grid(output_images, batch_size, user.n_rows, round_down=False)\n",
        "      output_images.insert(0, grid)\n",
        "\n",
        "  if user.sampler is not None:\n",
        "    del user.sampler\n",
        "  return output_images\n",
        "  "
      ],
      "metadata": {
        "id": "UITYO-1yLN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Legacy"
      ],
      "metadata": {
        "id": "iANyAC7nYCII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dream(prompt: str, ddim_steps: int, sampler_name: str, n_rows: int, ddim_eta: float, n_iter: int, n_samples: int, cfg_scale: float, seed: int, height: int, width: int, skip_save: bool, skip_grid: bool):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if seed == -1:\n",
        "        seed = random.randrange(4294967294)\n",
        "\n",
        "    seed = int(seed)\n",
        "    keep_same_seed = False\n",
        "\n",
        "    if sampler_name == 'PLMS':\n",
        "        sampler = PLMSSampler(model)\n",
        "    elif sampler_name == 'DDIM':\n",
        "        sampler = DDIMSampler(model)\n",
        "    elif sampler_name == 'k-diffusion':\n",
        "        sampler = KDiffusionSampler(model)\n",
        "    else:\n",
        "        raise Exception(\"Unknown sampler: \" + sampler_name)\n",
        "\n",
        "    batch_size = n_samples\n",
        "\n",
        "    assert prompt is not None\n",
        "    prompts = batch_size * [prompt]\n",
        "\n",
        "    precision_scope = autocast \n",
        "    output_images = []\n",
        "    with torch.no_grad(), precision_scope(\"cuda\"), model.ema_scope():\n",
        "        for n in range(n_iter):\n",
        "            \n",
        "            uc = None\n",
        "            if cfg_scale != 1.0:\n",
        "                uc = model.get_learned_conditioning(len(prompts) * [\"\"])\n",
        "            if isinstance(prompts, tuple):\n",
        "                prompts = list(prompts)\n",
        "            c = model.get_learned_conditioning(prompts)\n",
        "            shape = [opt_C, height // opt_f, width // opt_f]\n",
        "\n",
        "            batch_seed = seed if keep_same_seed else seed + n * len(prompts)\n",
        "\n",
        "            # we manually generate all input noises because each one should have a specific seed\n",
        "            x = create_random_tensors(batch_seed, shape, count=len(prompts), same_seed=keep_same_seed)\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps, conditioning=c, batch_size=len(prompts), shape=shape, verbose=False, unconditional_guidance_scale=cfg_scale, unconditional_conditioning=uc, eta=ddim_eta, x_T=x)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "            invalid_filename_chars = prompt\n",
        "            if not skip_save or not skip_grid:\n",
        "                for i, x_sample in enumerate(x_samples_ddim):\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    x_sample = x_sample.astype(np.uint8)\n",
        "\n",
        "                    image = Image.fromarray(x_sample)\n",
        "\n",
        "                    output_images.append(image)\n",
        "\n",
        "        if not skip_grid:\n",
        "            # additionally, save as grid\n",
        "            grid = image_grid(output_images, batch_size, n_rows, round_down=False)\n",
        "\n",
        "            output_images.insert(0, grid)\n",
        "\n",
        "    if sampler is not None:\n",
        "        del sampler\n",
        "    \n",
        "    return output_images"
      ],
      "metadata": {
        "id": "DsB_jx_QMM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translation(prompt: str, init_img, resize_mode: int, ddim_steps: int, n_rows:int, ddim_eta: float, n_iter: int, n_samples: int, cfg_scale: float, denoising_strength: float, seed: int, height: int, width: int, skip_save, skip_grid):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if seed == -1:\n",
        "        seed = random.randrange(4294967294)\n",
        "\n",
        "    model_wrap = K.external.CompVisDenoiser(model)\n",
        "\n",
        "    batch_size = n_samples\n",
        "\n",
        "    assert prompt is not None\n",
        "\n",
        "    image = init_img.convert(\"RGB\")\n",
        "    image = resize_image(resize_mode, image, width, height)\n",
        "    # image = image.resize((width, height), resample=PIL.Image.Resampling.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "\n",
        "    output_images = []\n",
        "    precision_scope = autocast\n",
        "    with torch.no_grad(), precision_scope(\"cuda\"), model.ema_scope():\n",
        "        init_image = 2. * image - 1.\n",
        "        init_image = init_image.to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "        x0 = init_latent\n",
        "\n",
        "        assert 0. <= denoising_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "        t_enc = int(denoising_strength * ddim_steps)\n",
        "\n",
        "        for n in range(n_iter):\n",
        "            prompts = batch_size * [prompt]\n",
        "\n",
        "            uc = None\n",
        "            if cfg_scale != 1.0:\n",
        "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "            if isinstance(prompts, tuple):\n",
        "                prompts = list(prompts)\n",
        "            c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "            batch_seed = seed + n * len(prompts)\n",
        "\n",
        "            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
        "            noise = create_random_tensors(batch_seed, x0.shape[1:], count=len(prompts))\n",
        "            noise = noise * sigmas[ddim_steps - t_enc - 1]\n",
        "\n",
        "            xi = x0 + noise\n",
        "            sigma_sched = sigmas[ddim_steps - t_enc - 1:]\n",
        "            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "\n",
        "            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, xi, sigma_sched, extra_args=extra_args, disable=False)\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "            if not skip_save or not skip_grid:\n",
        "                for i, x_sample in enumerate(x_samples_ddim):\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    x_sample = x_sample.astype(np.uint8)\n",
        "\n",
        "                    image = Image.fromarray(x_sample)\n",
        "\n",
        "                    output_images.append(image)\n",
        "\n",
        "        if not skip_grid:\n",
        "            # additionally, save as grid\n",
        "            grid = image_grid(output_images, batch_size, n_rows, round_down=False)\n",
        "            output_images.insert(0, grid)\n",
        "\n",
        "    return output_images"
      ],
      "metadata": {
        "id": "WMtx0PgGSaE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text 2 Image\n",
        "\n",
        "512x512 works well, 640x640 might crash colab"
      ],
      "metadata": {
        "id": "thSWMltDLYNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def txt2img(prompt: str, seed: int, sampler_name: str, steps: int, scale: int, strength: int, samples: int, rows: int, height: int, width: int, skip_grid: bool):\n",
        "  user = info()\n",
        "\n",
        "  user.prompt = prompt\n",
        "  user.seed = seed\n",
        "\n",
        "  if sampler_name == 'PLMS':\n",
        "    user.sampler = PLMSSampler(model)\n",
        "  elif sampler_name == 'DDIM':\n",
        "    user.sampler = DDIMSampler(model)\n",
        "  elif sampler_name == 'k-diffusion':\n",
        "    user.sampler = KDiffusionSampler(model)\n",
        "\n",
        "  user.ddim_steps = steps\n",
        "  user.cfg_scale = scale\n",
        "  user.strength = strength\n",
        "\n",
        "  user.n_samples = samples\n",
        "  user.n_rows = rows\n",
        "\n",
        "\n",
        "  user.height = height\n",
        "  user.width = width\n",
        "\n",
        "  user.skip_grid = skip_grid\n",
        "  user.n_iter = 1\n",
        "\n",
        "  return generate(user, \"txt2img\", None)"
      ],
      "metadata": {
        "id": "uubEebhcYXWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\\uD83C\\uDF08\\uD83E\\uDD16\\uD83E\\uDD16, octane 3D render\" #@param {type:\"string\"}\n",
        "sampler = 'k-diffusion' #@param [\"k-diffusion\", \"PLMS\", \"DDIM\"] {allow-input: false}\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "scale = 7 #@param {type:'integer'}\n",
        "steps = 2 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "samples = 4 #@param {type:'integer'}\n",
        "skip_grid = False #@param {type:\"boolean\"}\n",
        "rows = 2 #@param {type:'integer'}\n",
        "\n",
        "seed = 51234 #@param {type:'integer'}\n",
        "\n",
        "images = txt2img(prompt, seed, sampler, steps, scale, 0, samples, rows, height, width, skip_grid)\n"
      ],
      "metadata": {
        "id": "nhhlpsMlai7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img2img(prompt: str, seed: int, sampler_name: str, steps: int, scale: int, strength: int, samples: int, rows: int, height: int, width: int, skip_grid: bool, init_image, mode: str):\n",
        "  user = info()\n",
        "\n",
        "  user.prompt = prompt\n",
        "  user.seed = seed\n",
        "\n",
        "  user.sampler = KDiffusionSampler(model)\n",
        "\n",
        "  user.ddim_steps = steps\n",
        "  user.cfg_scale = scale\n",
        "  user.strength = strength\n",
        "\n",
        "  user.n_samples = samples\n",
        "  user.n_rows = rows\n",
        "\n",
        "  user.height = height\n",
        "  user.width = width\n",
        "\n",
        "  user.skip_grid = skip_grid\n",
        "  user.n_iter = 1\n",
        "\n",
        "  if mode == \"Just resize\":\n",
        "    user.resize_mode = 0\n",
        "  elif mode == \"Crop and resize\":\n",
        "    user.resize_mode = 1\n",
        "  else:\n",
        "    user.resize_mode = 2\n",
        "  \n",
        "  return generate(user, \"img2img\", init_image)"
      ],
      "metadata": {
        "id": "cjI1nW94e18B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a girl in a blue dress, wearing a white nurse cap, fair skin, long dark hair. At night, starry sky, bright light from the big yellowish moon. The girt is reaching out to the moon with her arms. Passionate, sad, depression, longing. Beautiful landscape background.Pastel painting by Josan Gonzalez, thomas kinkade, trending on artstation, cgsociety, deviantart, f/22, 35mm, graphic novel style, bleak, dark, ominous, threatening, tumultuous, Post-apocalyptic, Memphis Group, 1980s, Extreme long shot, Bokeh, 11pm, moon light, Kodachrome, cinestill 800t\" #@param {type:\"string\"}\n",
        "init_image_path = \"/content/stable-diffusion/Source/bad_0300X.png\" #@param {type: 'string'}\n",
        "resize_mode = \"Just resize\" #@param [\"Just resize\", \"Crop and resize\", \"Resize and fill\"] {allow-input: false}\n",
        "\n",
        "init_image = Image.open(init_image_path)\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "scale = 15 #@param {type:'integer'}\n",
        "steps = 100 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "strength = 0.99 #@param {type: \"slider\", min:0.00, max:1.00, step:0.01}\n",
        "\n",
        "samples = 4 #@param {type:'integer'}\n",
        "skip_grid = False #@param {type:\"boolean\"}\n",
        "rows = 2 #@param {type:'integer'}\n",
        "\n",
        "seed = 23082022 #@param {type:'integer'}\n",
        "\n",
        "sampler = 'k-diffusion' \n",
        "images = img2img(prompt, seed, sampler, steps, scale, strength, samples, rows, height, width, skip_grid, init_image, resize_mode)\n"
      ],
      "metadata": {
        "id": "sOFlRCFjfNf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {scale}, steps = {steps}, strength = {strength}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "e5i1NKmCU8z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Clear GPU if OOM (Run from set up again)"
      ],
      "metadata": {
        "id": "F73YlrzHQxNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "akuObYB5Mv8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual outputs"
      ],
      "metadata": {
        "id": "UeAcuHx-Em0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "VwiRZr9xMxd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "gaR2KDdoMy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "7-YR0dsdM0JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "cusiIi8SM1Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image 2 Image"
      ],
      "metadata": {
        "id": "NwsSUAbMLe04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a girl in a blue dress, wearing a white nurse cap, fair skin, long dark hair. At night, starry sky, bright light from the big yellowish moon. The girt is reaching out to the moon with her arms. Passionate, sad, depression, longing. Beautiful landscape background.Pastel painting by Josan Gonzalez, thomas kinkade, trending on artstation, cgsociety, deviantart, f/22, 35mm, graphic novel style, bleak, dark, ominous, threatening, tumultuous, Post-apocalyptic, Memphis Group, 1980s, Extreme long shot, Bokeh, 11pm, moon light, Kodachrome, cinestill 800t\" #@param {type:\"string\"}\n",
        "init_image_path = \"/content/stable-diffusion/Source/bad_0300X.png\" #@param {type: 'string'}\n",
        "resize_mode = \"Just resize\" #@param [\"Just resize\", \"Crop and resize\", \"Resize and fill\"] {allow-input: false}\n",
        "\n",
        "init_image = Image.open(init_image_path)\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7 #@param {type:'integer'}\n",
        "n_steps = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "strength = 0.99 #@param {type: \"slider\", min:0.00, max:1.00, step:0.01}\n",
        "ddim_eta = 0.0\n",
        "n_iter = 1\n",
        "n_samples = 4 #@param {type:'integer'}\n",
        "grid = True #@param {type:\"boolean\"}\n",
        "n_rows = 2 #@param {type:'integer'}\n",
        "\n",
        "seed = 2043630622222 #@param {type:'integer'}\n",
        "\n",
        "mode = 0\n",
        "if resize_mode == \"Crop and resize\":\n",
        "  mode = 1\n",
        "if resize_mode == \"Resize and fill\":\n",
        "  mode = 2\n",
        "\n",
        "\n",
        "images = translation(prompt, init_image, mode, n_steps, n_rows, ddim_eta, n_iter, n_samples, cfg_scale, strength, seed, height, width, True, not(grid))\n"
      ],
      "metadata": {
        "id": "SHOh80nzT4jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {cfg_scale}, steps = {n_steps}, strength = {strength}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "AfXX5n1TmBEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Clear GPU if OOM or funny errors (Run from set up again)"
      ],
      "metadata": {
        "id": "mcDhMNZGVoyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "LLKFosZiVpqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual results"
      ],
      "metadata": {
        "id": "7fplwcZ_EZaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "sEG1FyrGVpgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "2e1NLC_BVpZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "1b6Qdz8BVpRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "xytXKAcmVpJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image 2 Image (Sequence)"
      ],
      "metadata": {
        "id": "4D_g45M-F9e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running I2I Sequence"
      ],
      "metadata": {
        "id": "8lQVGJJFGGcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import clear_output \n",
        "import numpy as np\n",
        "import timeit\n",
        "#@title Image sequence\n",
        "#@markdown ##### Enter text prompt:\n",
        "prompt = \"man standing in front of a painting of a mountain in the distance, wide angle, Van Gogh, futuristic, surreal, holy\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ##### Set sequence duration (/Source/prefix_XXXXX.png):\n",
        "start_frame = \"00024\" #@param {type:\"string\"}\n",
        "end_frame = \"00037\" #@param {type:\"string\"}\n",
        "prefix = \"take\" #@param {type:\"string\"}\n",
        "current_frame = start_frame\n",
        "#@markdown ---\n",
        "#@markdown ##### SD settings:\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7 #@param {type:'integer'}\n",
        "n_steps = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "strength = 0.99 #@param {type: \"slider\", min:0.00, max:1.00, step:0.01}\n",
        "ddim_eta = 0.0\n",
        "n_iter = 1\n",
        "n_samples = 1\n",
        "grid = False\n",
        "n_rows = 1\n",
        "\n",
        "seed = 2043630622222 #@param {type:'integer'}\n",
        "\n",
        "timeStart = timeit.default_timer()\n",
        "for i in range(int(start_frame), int(end_frame) + 1):\n",
        "  current_frame = \"{:05d}\".format(i)\n",
        "  print(\"PROCESSING \" + current_frame)\n",
        "\n",
        "  init_image_path = f\"/content/stable-diffusion/Source/{prefix}_{current_frame}\"\n",
        "  init_image = Image.open(init_image_path)\n",
        "\n",
        "  \n",
        "  images[0].save(f\"/content/stable-diffusion/Output/{prefix}_{current_frame}.png\")\n",
        "  images[0]\n",
        "  images = translation(prompt, init_image, n_steps, n_rows, ddim_eta, n_iter, n_samples, cfg_scale, strength, seed, height, width, True, True)\n",
        "  clear_output()\n",
        "timeStop = timeit.default_timer()\n",
        "\n",
        "print(\"{} frames done in {} seconds.\\nPrompt: {}\\nStrength: {}\".format(int(end_frame) - int(start_frame), timeStop - timeStart, prompt, strength))"
      ],
      "metadata": {
        "id": "-LUoUqavGM2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export video"
      ],
      "metadata": {
        "id": "mtsnExYRGKBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export video\n",
        "from google.colab import files\n",
        "file_name = \"out.mp4\" #@param {type: \"string\"}\n",
        "always_overwrite = True #@param {type:\"boolean\"}\n",
        "framerate = 30 #@param (type:number)\n",
        "if always_overwrite:\n",
        "  replace = 'y'\n",
        "else:\n",
        "  replace = 'n'\n",
        "!yes {replace} | ffmpeg -framerate {framerate} -start_number {int(start_frame)} -i /content/stable-diffusion/Output/{prefix}_%05d.png -c:v libx264 -crf 0 {file_name}\n",
        "files.download('/content/stable-diffusion/{}'.format(file_name))"
      ],
      "metadata": {
        "id": "7rK2voJIHOt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving outputs"
      ],
      "metadata": {
        "id": "uZ13WzesoXdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/stable-diffusion/Output/\"\n",
        "name = \"out\" #@param {type:\"string\"}\n",
        "\n",
        "save_all = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_all:\n",
        "  k = 0\n",
        "  for i in images:\n",
        "    i.save(f'{path}{name}_{k}.png')\n",
        "    k += 1\n",
        "else:\n",
        "  index = 1 #@param {type:\"integer\"}\n",
        "  images[index].save(f'{path}{name}_{index}.png')"
      ],
      "metadata": {
        "id": "W_xIkXoMB9iQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jwZ0GT0eObBW",
        "XvHTXI7KOnu4",
        "yaHfjOHyJgdC",
        "N-ObhCcRHhFv",
        "Oudkt8AEKw05",
        "OmQfJWm8QGoA",
        "-7syo80FPa4T",
        "ihQD4BD1P1Vr",
        "P95iTXcZQbnk",
        "c6jepvv8Q-1a",
        "IFdeC2LZRBm-",
        "7oPo2rboRWfH",
        "NMj_JbQ9Rrxq",
        "iIBkjkG_dHAu",
        "5kQ2bDBrPe6Q",
        "YbgZNNTxLOEo",
        "fVPP3LONLmdN",
        "na17SiCBMY9Y",
        "thSWMltDLYNy",
        "UeAcuHx-Em0U",
        "NwsSUAbMLe04",
        "8lQVGJJFGGcK",
        "uZ13WzesoXdq"
      ],
      "name": "Neo Hidamari Diffusion.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}