{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FuouM/HidamariDiffusionColab/blob/main/Neo_Hidamari_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtJ1grxqfG8u"
      },
      "source": [
        "Code taken from https://colab.research.google.com/drive/1yf3-bUhTcfxRmAphJHVQQAD2ArYO1CRZ\n",
        "\n",
        "Guides:\n",
        "\n",
        "https://rentry.org/retardsguide - Main txt2img guide\n",
        "https://rentry.org/kretard - K-Diffusion guide\n",
        "https://rentry.org/img2img - img2img guide\n",
        "\n",
        "Updated to use the new model with old code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "dTlNpVz7d_v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwZ0GT0eObBW"
      },
      "source": [
        "### Download and set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTjD9Ij7Nuh4"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/basujindal/stable-diffusion\n",
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cdn.discordapp.com/attachments/1011261946223394877/1012027527792951377/NotoSansJP-Bold.otf"
      ],
      "metadata": {
        "id": "HtnJTmg8SIrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pxSsF1HOi4V"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install albumentations==0.4.3\n",
        "!pip install opencv-python==4.1.2.30\n",
        "!pip install pudb==2019.2\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.2\n",
        "#!pip install pytorch-lightning==1.4.2\n",
        "!pip install pytorch-lightning \n",
        "!pip install omegaconf==2.1.1\n",
        "!pip install test-tube>=0.7.5\n",
        "!pip install streamlit>=0.73.1\n",
        "!pip install einops==0.3.0\n",
        "!pip install torch-fidelity==0.3.0\n",
        "# !pip install pilmoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQILJaTuZCLU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.19.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52gTy8xUZD_5"
      },
      "outputs": [],
      "source": [
        "!yes w | pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "!yes w | pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git src/k-diffusion\n",
        "!pip install src/k-diffusion\n",
        "!pip install kornia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPIqZIAu_SNj"
      },
      "outputs": [],
      "source": [
        "!mkdir -p '/content/stable-diffusion/Source'\n",
        "!mkdir -p '/content/stable-diffusion/Output'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnULtFtROkqr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # This will crash Colab (required, everything will still be intact so dont worry)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHTXI7KOnu4"
      },
      "source": [
        "### Download the model (Choose only 1 option)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAw5HFJRO4b-"
      },
      "outputs": [],
      "source": [
        "!gdown 1athuOO6kKtvLm3x1zqxNN8fqBIzV15ss -O /content/stable-diffusion/model.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1CqjnK1Ik0J"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion/\n",
        "!wget -O model.ckpt \"https://drive.yerf.org/wl/?id=EBfTrmcCCUAGaQBXVIj5lJmEhjoP1tgl&mode=grid&download=1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For Gdrive"
      ],
      "metadata": {
        "id": "yaHfjOHyJgdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "s1CMRG1FJo2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_path = \"\" #@param {type: 'string'}\n",
        "file_name = \"sd-v1-4.ckpt\" #@param {type: 'string'}\n",
        "%cd {user_path}\n",
        "\n",
        "!cp {file_name} \"/content/stable-diffusion/\"\n",
        "\n",
        "%cd /content/stable-diffusion\n",
        "!mv {file_name} model.ckpt"
      ],
      "metadata": {
        "id": "rb-O9TBEJt2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Huggingface"
      ],
      "metadata": {
        "id": "N-ObhCcRHhFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_token = \"\" #@param {type:\"string\"}\n",
        "user_header = f\"\\\"Authorization: Bearer {user_token}\\\"\"\n",
        "!wget --header={user_header} https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt -O /content/stable-diffusion/model.ckpt"
      ],
      "metadata": {
        "id": "3BQoPx_8Hj08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimized SD"
      ],
      "metadata": {
        "id": "Oudkt8AEKw05"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmQfJWm8QGoA"
      },
      "source": [
        "## Text 2 Image (Optimized SD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7syo80FPa4T"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY4lXPAhbU88"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgwY7MOYXfja"
      },
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvnfjJvQXjiY"
      },
      "outputs": [],
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qHWyM4qPfON"
      },
      "outputs": [],
      "source": [
        "def generate(opt,prompt,grid):\n",
        "    device = 'cuda'\n",
        "    images = []\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "            for prompts in tqdm(data, desc=\"data\"):\n",
        "                with precision_scope(\"cuda\"):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    samples_ddim = model.sample(S=opt.ddim_steps,\n",
        "                                                conditioning=c,\n",
        "                                                batch_size=opt.n_samples,\n",
        "                                                seed = opt.seed,\n",
        "                                                shape=shape,\n",
        "                                                verbose=False,\n",
        "                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                unconditional_conditioning=uc,\n",
        "                                                eta=opt.ddim_eta,\n",
        "                                                x_T=start_code)\n",
        "\n",
        "                    modelFS.to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    if grid:\n",
        "                        all_samples.append(x_samples_ddim)\n",
        "\n",
        "                    del samples_ddim\n",
        "                    print(\"memory_final = \", torch.cuda.memory_allocated()/1e6)\n",
        "            if grid:\n",
        "                grid = torch.stack(all_samples, 0)\n",
        "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                grid = make_grid(grid, nrow=n_rows)\n",
        "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "            print(f'Finished!')\n",
        "            return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o5sCHQ2X3vc"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "km8NTDiXXm4Z"
      },
      "outputs": [],
      "source": [
        "ckpt = '/content/stable-diffusion/model.ckpt'\n",
        "\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li = []\n",
        "lo = []\n",
        "for key, value in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihQD4BD1P1Vr"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcSFxhq4P8kl"
      },
      "outputs": [],
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 1448\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 256 # Height, the vertical resolution\n",
        "        self.W = 256 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 9 # Amount of images outputted\n",
        "        self.n_rows = 3 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)\n",
        "\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "\n",
        "if opt.precision == \"autocast\":\n",
        "    model.half()\n",
        "    modelCS.half()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95iTXcZQbnk"
      },
      "source": [
        "### Run Text 2 Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmNXWU20QesI"
      },
      "outputs": [],
      "source": [
        "prompt = \"A blue corgi. At the table. Drinking tea. Next to the window. Dusk. Sun shines through the window. Light reflection on table.\" #@param {type:\"string\"}\n",
        "scale = 15 #@param {type:\"number\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "\n",
        "n_steps = 50 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "opt.seed = 12345678 #@param {type:\"integer\"};\n",
        "seed_everything(opt.seed)\n",
        "grid = \"no\"\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "images = generate(opt=opt, prompt=prompt, grid=(grid==\"no\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeaR0VMfr_im"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / Funny errors (Run from Set up again)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwJbeqpZMYs"
      },
      "source": [
        "Save images as a 4x4 collage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R400-g0uSmYT"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "def collage_maker(image1, image2, image3, image4, name):\n",
        "    col_1 = np.vstack([image1, image2])\n",
        "    col_2 = np.vstack([image3, image4])\n",
        "    collage = np.hstack([col_1, col_2])\n",
        "    image = Image.fromarray(collage)\n",
        "    image.save(name)\n",
        "\n",
        "collage_maker(images[1], images[2], images[3], images[4], \"nice.png\")\n",
        "from IPython.display import Image\n",
        "Image(filename='nice.png') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDQmNv_9ZTGc"
      },
      "source": [
        "#### Save images individually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IAzahTMZVIE"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8AwaB3XZhfb"
      },
      "outputs": [],
      "source": [
        "images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWys2AGRZhVg"
      },
      "outputs": [],
      "source": [
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e72kpFjNZhL8"
      },
      "outputs": [],
      "source": [
        "images[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2D4kZNNJ_Uv"
      },
      "outputs": [],
      "source": [
        "images[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jepvv8Q-1a"
      },
      "source": [
        "## Image 2 Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdeC2LZRBm-"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw-VyYfuRBPn"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion\n",
        "\n",
        "import argparse, os, sys, glob\n",
        "import PIL\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "from torchvision.utils import make_grid\n",
        "from torch import autocast\n",
        "from contextlib import nullcontext\n",
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_img(path):\n",
        "    image = Image.open(path).convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h}) from {path}\")\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image).half()\n",
        "    return 2.*image - 1.\n",
        "\n",
        "def generate(opt,init_img,grid,prompt):\n",
        "    device = 'cuda'\n",
        "    images = []\n",
        "    all_samples = list()\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    assert os.path.isfile(init_img)\n",
        "    init_image = load_img(init_img).to(device)\n",
        "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "    print(f\"target t_enc is {t_enc} steps\")\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
        "                    for prompts in tqdm(data, desc=\"data\"):\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                                  unconditional_conditioning=uc,)\n",
        "\n",
        "                        if grid:\n",
        "                            all_samples.append(torch.clamp((model.decode_first_stage(samples) + 1.0) / 2.0, min=0.0, max=1.0))\n",
        "\n",
        "                        for i in range(batch_size):\n",
        "                            x_samples_ddim = model.decode_first_stage(samples[i].unsqueeze(0))\n",
        "                            x_sample = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                            x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                            images += [Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                        del samples\n",
        "    if grid:\n",
        "        grid = torch.stack(all_samples, 0)\n",
        "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "        grid = make_grid(grid, nrow=n_rows)\n",
        "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "        images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "    print(f'Finished!')\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oPo2rboRWfH"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pAF_LzRZSr"
      },
      "outputs": [],
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = '/content/stable-diffusion/model.ckpt' # MODEL PATH\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 7777\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 512 # Height, the vertical resolution\n",
        "        self.W = 512 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "        self.strength = 0.7 # How agressive it is, keep between 0.2 ~ 1.0\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 9 # Amount of images outputted\n",
        "        self.n_rows = 3 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)\n",
        "\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\").half()\n",
        "sampler = DDIMSampler(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMj_JbQ9Rrxq"
      },
      "source": [
        "### Run Image 2 Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdR7s1yQR3m4"
      },
      "outputs": [],
      "source": [
        "%cd '/content/stable-diffusion/Source'\n",
        "link = \"https://cdn.discordapp.com/attachments/692612547345514549/1010734092687519744/take_00474.png\" #@param {type:\"string\"}\n",
        "!wget {link}\n",
        "%cd  '/content/stable-diffusion/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q1Qeg3_RwKU"
      },
      "outputs": [],
      "source": [
        "prompt = \"a male hand holding a gun shooting a bullet with gun powder and explosion, hyperreal, artstation, masterpiece, oil painting, close shot\" #@param {type:\"string\"}\n",
        "img = \"/content/stable-diffusion/Source/take_00474.png\" #@param {type:\"string\"}\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 384 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.99 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 4 #@param {type:\"integer\"}\n",
        "n_rows = 2 #@param {type:\"integer\"}\n",
        "opt.seed = 204363062 #@param {type:\"integer\"};\n",
        "grid = \"no\"\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "opt.strength = strength\n",
        "images = generate(opt, init_img=img, grid=(grid==\"no\"), prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3zuCpuYNJ6_"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgSuZn4DOKAs"
      },
      "outputs": [],
      "source": [
        "images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rHaSK3VP6ja"
      },
      "outputs": [],
      "source": [
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldZcKISoP8E5"
      },
      "outputs": [],
      "source": [
        "images[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ1-eVNhP8-k"
      },
      "outputs": [],
      "source": [
        "images[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kQ2bDBrPe6Q"
      },
      "source": [
        "### Running image sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DtBv6OVPiR_"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from IPython.display import clear_output \n",
        "import numpy as np\n",
        "import timeit\n",
        "#@title Image sequence\n",
        "#@markdown ##### Enter text prompt:\n",
        "prompt = \"man standing in front of a painting of a mountain in the distance, wide angle, Van Gogh, futuristic, surreal, holy\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ##### Set sequence duration (/Source/prefix_XXXXX.png):\n",
        "start_frame = \"00024\" #@param {type:\"string\"}\n",
        "end_frame = \"00037\" #@param {type:\"string\"}\n",
        "prefix = \"take\" #@param {type:\"string\"}\n",
        "current_frame = start_frame\n",
        "#@markdown ---\n",
        "#@markdown ##### SD settings:\n",
        "scale = 5 #@param {type:\"number\"}\n",
        "height = 384 #@param {type:\"integer\"}\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "n_steps = 100 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "strength = 0.7 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "n_images = 1 #@param {type:\"integer\"}\n",
        "n_rows = 1 #@param {type:\"integer\"}\n",
        "grid = \"no\"\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "opt.strength = strength\n",
        "opt.seed = 204363062 #@param {type:\"integer\"};\n",
        "timeStart = timeit.default_timer()\n",
        "for i in range(int(start_frame), int(end_frame) + 1):\n",
        "  current_frame = \"{:05d}\".format(i)\n",
        "  print(\"PROCESSING \" + current_frame)\n",
        "  img = \"/content/stable-diffusion/Source/{}_{}.png\".format(prefix, current_frame)\n",
        "  images = generate(opt, init_img=img, grid=(grid==\"no\"), prompt=prompt)\n",
        "  \n",
        "  images[0].save(\"/content/stable-diffusion/Output/{}_{}.png\".format(prefix, current_frame))\n",
        "  images[0]\n",
        "\n",
        "  clear_output()\n",
        "timeStop = timeit.default_timer()\n",
        "print(\"{} frames done in {} seconds.\\nPrompt: {}\\nStrength: {}\".format(int(end_frame) - int(start_frame), timeStop - timeStart, prompt, strength))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1cRK744Ch4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / Funny errors (Run from Set up again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFnTkna0dP1K"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "!zip -r \"/content/stable-diffusion/result.zip\" \"/content/stable-diffusion/Output\"\n",
        "files.download('/content/stable-diffusion/result.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBmrTroGjzxt"
      },
      "outputs": [],
      "source": [
        "!rm \"/content/stable-diffusion/result.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwtcgOzRhll5"
      },
      "outputs": [],
      "source": [
        "#@title Export video\n",
        "from google.colab import files\n",
        "file_name = \"out.mp4\" #@param {type: \"string\"}\n",
        "always_overwrite = True #@param {type:\"boolean\"}\n",
        "framerate = 30 #@param (type:number)\n",
        "if always_overwrite:\n",
        "  replace = 'y'\n",
        "else:\n",
        "  replace = 'n'\n",
        "!yes {replace} | ffmpeg -framerate {framerate} -start_number {int(start_frame)} -i /content/stable-diffusion/Output/{prefix}_%05d.png -c:v libx264 -crf 0 {file_name}\n",
        "files.download('/content/stable-diffusion/{}.mp4'.format(file_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fViEXufPjam6"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/stable-diffusion/Source/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np75eoywjbn8"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/stable-diffusion/Output/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTdaBTHeCpUr"
      },
      "source": [
        "## Text 2 Image (K-diffusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "livJxlGVCwCl"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWNg7ZjdCs-H"
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ozn7ByC5Cyxl"
      },
      "outputs": [],
      "source": [
        "import argparse, os, sys, glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "import accelerate\n",
        "import k_diffusion as K\n",
        "import torch.nn as nn\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg8r2az5Cz3L"
      },
      "outputs": [],
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "\n",
        "def load_model_from_config(ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cuda:0\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    return sd\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "def generate(opt,prompt,grid):\n",
        "    accelerator = accelerate.Accelerator()\n",
        "    device = accelerator.device\n",
        "    images = []\n",
        "\n",
        "    seeds = torch.randint(-2 ** 63, 2 ** 63 - 1, [accelerator.num_processes])\n",
        "    torch.manual_seed(seeds[accelerator.process_index].item())\n",
        "\n",
        "    batch_size = opt.n_samples\n",
        "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
        "\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "\n",
        "    start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        all_samples = list()\n",
        "        with precision_scope(\"cuda\"):\n",
        "            for n in trange(opt.n_iter, desc=\"Sampling\", disable =not accelerator.is_main_process):\n",
        "                for prompts in tqdm(data, desc=\"data\", disable =not accelerator.is_main_process):\n",
        "                    modelCS.to(device)\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = modelCS.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "\n",
        "                    c = modelCS.get_learned_conditioning(prompts)\n",
        "                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelCS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                    torch.manual_seed(opt.seed)\n",
        "                    x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0] # for GPU draw\n",
        "                    model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                    extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                    samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=not accelerator.is_main_process)\n",
        "                    \n",
        "                    modelFS.to(device)\n",
        "                    for i in range(batch_size):\n",
        "                        x_samples_ddim = modelFS.decode_first_stage(samples_ddim[i].unsqueeze(0))\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "                        x_sample = accelerator.gather(x_samples_ddim)\n",
        "                        if grid:\n",
        "                            all_samples.append(x_sample)\n",
        "                        x_sample = 255. * rearrange(x_sample[0].cpu().numpy(), 'c h w -> h w c')\n",
        "                        images +=[Image.fromarray(x_sample.astype(np.uint8))]\n",
        "                    mem = torch.cuda.memory_allocated()/1e6\n",
        "                    modelFS.to(\"cpu\")\n",
        "                    while(torch.cuda.memory_allocated()/1e6 >= mem):\n",
        "                        time.sleep(1)\n",
        "\n",
        "                    del samples_ddim\n",
        "            if grid:\n",
        "                grid = torch.stack(all_samples, 0)\n",
        "                grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
        "                grid = make_grid(grid, nrow=n_rows)\n",
        "                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
        "                images = [Image.fromarray(grid.astype(np.uint8))] + images\n",
        "\n",
        "            print(f'Finished!')\n",
        "            return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2whOicXWqzn"
      },
      "outputs": [],
      "source": [
        "ckpt = '/content/stable-diffusion/model.ckpt'\n",
        "\n",
        "sd = load_model_from_config(f\"{ckpt}\")\n",
        "li = []\n",
        "lo = []\n",
        "for key, value in sd.items():\n",
        "    sp = key.split('.')\n",
        "    if(sp[0]) == 'model':\n",
        "        if('input_blocks' in sp):\n",
        "            li.append(key)\n",
        "        elif('middle_block' in sp):\n",
        "            li.append(key)\n",
        "        elif('time_embed' in sp):\n",
        "            li.append(key)\n",
        "        else:\n",
        "            lo.append(key)\n",
        "for key in li:\n",
        "    sd['model1.' + key[6:]] = sd.pop(key)\n",
        "for key in lo:\n",
        "    sd['model2.' + key[6:]] = sd.pop(key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJsqY6L3C12V"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUJ4HsrMC4Yn"
      },
      "outputs": [],
      "source": [
        "class config():\n",
        "      def __init__(self):\n",
        "        self.config = 'optimizedSD/v1-inference.yaml' # Don't change this\n",
        "        self.ckpt = ckpt # If you want to change the model location, change it on the Load movel section\n",
        "\n",
        "        self.precision = 'autocast' # Change to full and fuck your RAM\n",
        "        self.ddim_eta = 0.0 # Does nothing, keep as is\n",
        "        self.C = 4 # Keep as is\n",
        "\n",
        "        self.seed = 204363062\n",
        "\n",
        "        self.ddim_steps = 30 # Keep within 30 ~ 250, higher is better but slower\n",
        "        self.H = 256 # Height, the vertical resolution\n",
        "        self.W = 256 # Width, the horizontal resolution\n",
        "        self.f = 8 # Visual scale maybe, 256x256 with f = 4 seems to use same RAM as 512x512 with f = 8\n",
        "        self.scale = 7.5 # Keep within 4 ~ 25, maybe, changes how the prompt is interpreted\n",
        "\n",
        "        self.n_iter = 1 # Maybe improves it, reccomended to keep as is as it multiplies the waiting time\n",
        "        self.n_samples = 4 # Amount of images outputted\n",
        "        self.n_rows = 2 # How many images per row (used on grid)\n",
        "\n",
        "opt = config()\n",
        "seed_everything(opt.seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_MXqg6PC5rx"
      },
      "outputs": [],
      "source": [
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "config.modelUNet.params.ddim_steps = opt.ddim_steps\n",
        "\n",
        "model = instantiate_from_config(config.modelUNet)\n",
        "_, _ = model.load_state_dict(sd, strict=False)\n",
        "model.eval()\n",
        "    \n",
        "modelCS = instantiate_from_config(config.modelCondStage)\n",
        "_, _ = modelCS.load_state_dict(sd, strict=False)\n",
        "modelCS.eval()\n",
        "    \n",
        "modelFS = instantiate_from_config(config.modelFirstStage)\n",
        "_, _ = modelFS.load_state_dict(sd, strict=False)\n",
        "modelFS.eval()\n",
        "\n",
        "if opt.precision == \"autocast\":\n",
        "    model.half()\n",
        "    modelCS.half()\n",
        "    modelFS.half()\n",
        "\n",
        "model_wrap = K.external.CompVisDenoiser(model)\n",
        "sigma_min, sigma_max = model_wrap.sigmas[0].item(), model_wrap.sigmas[-1].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taV6Yr1KC_o1"
      },
      "source": [
        "### Run k-diffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fJgAMPhGC_O1"
      },
      "outputs": [],
      "source": [
        "prompt = \"Ukiyo-e painting of a cat hacker wearing VR headsets, on a postage stamp\" #@param {type:\"string\"}\n",
        "scale = 15 #@param {type:\"number\"}\n",
        "height = 640 #@param {type:\"integer\"}\n",
        "width = 640 #@param {type:\"integer\"}\n",
        "n_steps = 50 #@param {type:\"slider\", min:30, max:250, step:5}\n",
        "n_images = 1 #@param {type:\"integer\"}\n",
        "n_rows = 1 #@param {type:\"integer\"}\n",
        "grid = \"yes\" #@param [\"yes\", \"no\"]\n",
        "opt.seed = 204363062 #@param {type:\"integer\"};\n",
        "seed_everything(opt.seed)\n",
        "opt.scale = scale\n",
        "opt.H = height\n",
        "opt.W = width\n",
        "opt.n_samples = n_images\n",
        "opt.n_rows = n_rows\n",
        "opt.ddim_steps = n_steps\n",
        "images = generate(opt=opt, prompt=prompt, grid=(grid==\"yes\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePoIOSmZDGDZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Crash colab if runs out of gpu memory / [type object 'Image' has no attribute 'fromarray'] (Run from Set up again)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeSHDW6QDGtC"
      },
      "outputs": [],
      "source": [
        "images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3Ze6xi8DHh7"
      },
      "outputs": [],
      "source": [
        "images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1r02bg4IDISM"
      },
      "outputs": [],
      "source": [
        "images[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8AsiCyGDJd4"
      },
      "outputs": [],
      "source": [
        "images[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUPmXE8mDJ8p"
      },
      "outputs": [],
      "source": [
        "images[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hlky fork (no GFPGAN) - Gradio-less\n",
        "\n",
        "https://github.com/hlky/stable-diffusion"
      ],
      "metadata": {
        "id": "YbgZNNTxLOEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ],
      "metadata": {
        "id": "fVPP3LONLmdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/stable-diffusion\n",
        "\n",
        "import argparse, os, sys, glob\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw\n",
        "from tqdm import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "from torch import autocast\n",
        "\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "import k_diffusion as K\n",
        "import torch.nn as nn\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "ZcXB2r-TLmSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_img_pil(img_pil):\n",
        "    image = img_pil.convert(\"RGB\")\n",
        "    w, h = image.size\n",
        "    print(f\"loaded input image of size ({w}, {h})\")\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    print(f\"cropped image to size ({w}, {h})\")\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2. * image - 1.\n",
        "\n",
        "\n",
        "def load_img(path):\n",
        "    return load_img_pil(Image.open(path))\n",
        "\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class KDiffusionSampler:\n",
        "    def __init__(self, m):\n",
        "        self.model = m\n",
        "        self.model_wrap = K.external.CompVisDenoiser(m)\n",
        "\n",
        "    def sample(self, S, conditioning, batch_size, shape, verbose, unconditional_guidance_scale, unconditional_conditioning, eta, x_T):\n",
        "        sigmas = self.model_wrap.get_sigmas(S)\n",
        "        x = x_T * sigmas[0]\n",
        "        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
        "        samples_ddim = K.sampling.sample_lms(model_wrap_cfg, x, sigmas, extra_args={'cond': conditioning, 'uncond': unconditional_conditioning, 'cond_scale': unconditional_guidance_scale}, disable=False)\n",
        "\n",
        "        return samples_ddim, None\n",
        "\n",
        "\n",
        "def create_random_tensors(seed, shape, count, same_seed=False):\n",
        "    xs = []\n",
        "    for i in range(count):\n",
        "        current_seed = seed if same_seed else seed + i\n",
        "        torch.manual_seed(current_seed)\n",
        "        xs.append(torch.randn(shape, device=device))\n",
        "    x = torch.stack(xs)\n",
        "    return x\n",
        "\n",
        "def image_grid(imgs, batch_size, n_rows:int, round_down=False):\n",
        "    if n_rows > 0:\n",
        "        rows = n_rows\n",
        "    elif n_rows == 0:\n",
        "        rows = batch_size\n",
        "    else:\n",
        "        rows = math.sqrt(len(imgs))\n",
        "        rows = int(rows) if round_down else round(rows)\n",
        "\n",
        "    cols = math.ceil(len(imgs) / rows)\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols * w, rows * h), color='black')\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "\n",
        "    return grid"
      ],
      "metadata": {
        "id": "3I78LM7iLX4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Updates"
      ],
      "metadata": {
        "id": "gkLTqrwyRcAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## UPDATE\n",
        "\n",
        "LANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n",
        "invalid_filename_chars = '<>:\"/\\|?*\\n'\n",
        "\n",
        "def resize_image(resize_mode, im, width, height):\n",
        "    if resize_mode == 0:\n",
        "        res = im.resize((width, height), resample=LANCZOS)\n",
        "    elif resize_mode == 1:\n",
        "        ratio = width / height\n",
        "        src_ratio = im.width / im.height\n",
        "\n",
        "        src_w = width if ratio > src_ratio else im.width * height // im.height\n",
        "        src_h = height if ratio <= src_ratio else im.height * width // im.width\n",
        "\n",
        "        resized = im.resize((src_w, src_h), resample=LANCZOS)\n",
        "        res = Image.new(\"RGB\", (width, height))\n",
        "        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n",
        "    else:\n",
        "        ratio = width / height\n",
        "        src_ratio = im.width / im.height\n",
        "\n",
        "        src_w = width if ratio < src_ratio else im.width * height // im.height\n",
        "        src_h = height if ratio >= src_ratio else im.height * width // im.width\n",
        "\n",
        "        resized = im.resize((src_w, src_h), resample=LANCZOS)\n",
        "        res = Image.new(\"RGB\", (width, height))\n",
        "        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n",
        "\n",
        "        if ratio < src_ratio:\n",
        "            fill_height = height // 2 - src_h // 2\n",
        "            res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n",
        "            res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\n",
        "        else:\n",
        "            fill_width = width // 2 - src_w // 2\n",
        "            res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n",
        "            res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "e3PYezWfOUYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageFont, ImageDraw \n",
        "\n",
        "def add_margin(pil_img, top, right, bottom, left, color):\n",
        "    width, height = pil_img.size\n",
        "    new_width = width + right + left\n",
        "    new_height = height + top + bottom\n",
        "    result = Image.new(pil_img.mode, (new_width, new_height), color)\n",
        "    result.paste(pil_img, (left, top))\n",
        "    return result\n",
        "\n",
        "def text_wrap(text, font, max_width):\n",
        "        lines = []\n",
        "        if font.getsize(text)[0]  <= max_width:\n",
        "            lines.append(text)\n",
        "        else:\n",
        "            words = text.split(' ')\n",
        "            i = 0\n",
        "            while i < len(words):\n",
        "                line = ''\n",
        "                while i < len(words) and font.getsize(line + words[i])[0] <= max_width:\n",
        "                    line = line + words[i]+ \" \"\n",
        "                    i += 1\n",
        "                if not line:\n",
        "                    line = words[i]\n",
        "                    i += 1\n",
        "                lines.append(line)\n",
        "        return lines\n",
        "\n",
        "def caption(image, prompt, info):\n",
        "  width, height = image.size\n",
        "\n",
        "  font = ImageFont.truetype(\"/content/stable-diffusion/NotoSansJP-Bold.otf\", 20, encoding='utf-8')\n",
        "  lines = text_wrap(prompt, font, image.size[0])\n",
        "  lines.append(f\"{info}\")\n",
        "  line_height = font.getsize('hg')[1]\n",
        "  cap_img = add_margin(image, 0, 0, line_height * (len(lines) + 1), 0, (255, 255, 255))\n",
        "  draw = ImageDraw.Draw(cap_img)\n",
        "  pad = 2\n",
        "  x = pad * 2\n",
        "  y = height + pad\n",
        "  for line in lines:\n",
        "      draw.text((x,y), line, fill=(0, 0, 0), font=font)\n",
        "      y = y + line_height\n",
        "  return cap_img"
      ],
      "metadata": {
        "id": "vqRs04yoRett"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "na17SiCBMY9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    ckpt = '/content/stable-diffusion/model.ckpt'\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "config = OmegaConf.load(\"configs/stable-diffusion/v1-inference.yaml\")\n",
        "model = load_model_from_config(config, \"models/ldm/stable-diffusion-v1/model.ckpt\")\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.half().to(device)"
      ],
      "metadata": {
        "id": "C8J52y2xMcti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt_C = 4\n",
        "opt_f = 8\n",
        "\n",
        "def dream(prompt: str, ddim_steps: int, sampler_name: str, n_rows: int, ddim_eta: float, n_iter: int, n_samples: int, cfg_scale: float, seed: int, height: int, width: int, skip_save: bool, skip_grid: bool):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if seed == -1:\n",
        "        seed = random.randrange(4294967294)\n",
        "\n",
        "    seed = int(seed)\n",
        "    keep_same_seed = False\n",
        "\n",
        "    if sampler_name == 'PLMS':\n",
        "        sampler = PLMSSampler(model)\n",
        "    elif sampler_name == 'DDIM':\n",
        "        sampler = DDIMSampler(model)\n",
        "    elif sampler_name == 'k-diffusion':\n",
        "        sampler = KDiffusionSampler(model)\n",
        "    else:\n",
        "        raise Exception(\"Unknown sampler: \" + sampler_name)\n",
        "\n",
        "    batch_size = n_samples\n",
        "\n",
        "    assert prompt is not None\n",
        "    prompts = batch_size * [prompt]\n",
        "\n",
        "    precision_scope = autocast \n",
        "    output_images = []\n",
        "    with torch.no_grad(), precision_scope(\"cuda\"), model.ema_scope():\n",
        "        for n in range(n_iter):\n",
        "            \n",
        "            uc = None\n",
        "            if cfg_scale != 1.0:\n",
        "                uc = model.get_learned_conditioning(len(prompts) * [\"\"])\n",
        "            if isinstance(prompts, tuple):\n",
        "                prompts = list(prompts)\n",
        "            c = model.get_learned_conditioning(prompts)\n",
        "            shape = [opt_C, height // opt_f, width // opt_f]\n",
        "\n",
        "            batch_seed = seed if keep_same_seed else seed + n * len(prompts)\n",
        "\n",
        "            # we manually generate all input noises because each one should have a specific seed\n",
        "            x = create_random_tensors(batch_seed, shape, count=len(prompts), same_seed=keep_same_seed)\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps, conditioning=c, batch_size=len(prompts), shape=shape, verbose=False, unconditional_guidance_scale=cfg_scale, unconditional_conditioning=uc, eta=ddim_eta, x_T=x)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "            invalid_filename_chars = prompt\n",
        "            if not skip_save or not skip_grid:\n",
        "                for i, x_sample in enumerate(x_samples_ddim):\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    x_sample = x_sample.astype(np.uint8)\n",
        "\n",
        "                    image = Image.fromarray(x_sample)\n",
        "\n",
        "                    output_images.append(image)\n",
        "\n",
        "        if not skip_grid:\n",
        "            # additionally, save as grid\n",
        "            grid = image_grid(output_images, batch_size, n_rows, round_down=False)\n",
        "\n",
        "            output_images.insert(0, grid)\n",
        "\n",
        "    if sampler is not None:\n",
        "        del sampler\n",
        "    \n",
        "    return output_images"
      ],
      "metadata": {
        "id": "DsB_jx_QMM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translation(prompt: str, init_img, resize_mode: int, ddim_steps: int, n_rows:int, ddim_eta: float, n_iter: int, n_samples: int, cfg_scale: float, denoising_strength: float, seed: int, height: int, width: int, skip_save, skip_grid):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if seed == -1:\n",
        "        seed = random.randrange(4294967294)\n",
        "\n",
        "    model_wrap = K.external.CompVisDenoiser(model)\n",
        "\n",
        "    batch_size = n_samples\n",
        "\n",
        "    assert prompt is not None\n",
        "\n",
        "    image = init_img.convert(\"RGB\")\n",
        "    image = resize_image(resize_mode, image, width, height)\n",
        "    # image = image.resize((width, height), resample=PIL.Image.Resampling.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "\n",
        "    output_images = []\n",
        "    precision_scope = autocast\n",
        "    with torch.no_grad(), precision_scope(\"cuda\"), model.ema_scope():\n",
        "        init_image = 2. * image - 1.\n",
        "        init_image = init_image.to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "        x0 = init_latent\n",
        "\n",
        "        assert 0. <= denoising_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
        "        t_enc = int(denoising_strength * ddim_steps)\n",
        "\n",
        "        for n in range(n_iter):\n",
        "            prompts = batch_size * [prompt]\n",
        "\n",
        "            uc = None\n",
        "            if cfg_scale != 1.0:\n",
        "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "            if isinstance(prompts, tuple):\n",
        "                prompts = list(prompts)\n",
        "            c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "            batch_seed = seed + n * len(prompts)\n",
        "\n",
        "            sigmas = model_wrap.get_sigmas(ddim_steps)\n",
        "            noise = create_random_tensors(batch_seed, x0.shape[1:], count=len(prompts))\n",
        "            noise = noise * sigmas[ddim_steps - t_enc - 1]\n",
        "\n",
        "            xi = x0 + noise\n",
        "            sigma_sched = sigmas[ddim_steps - t_enc - 1:]\n",
        "            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': cfg_scale}\n",
        "\n",
        "            samples_ddim = K.sampling.sample_lms(model_wrap_cfg, xi, sigma_sched, extra_args=extra_args, disable=False)\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "            if not skip_save or not skip_grid:\n",
        "                for i, x_sample in enumerate(x_samples_ddim):\n",
        "                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                    x_sample = x_sample.astype(np.uint8)\n",
        "\n",
        "                    image = Image.fromarray(x_sample)\n",
        "\n",
        "                    output_images.append(image)\n",
        "\n",
        "        if not skip_grid:\n",
        "            # additionally, save as grid\n",
        "            grid = image_grid(output_images, batch_size, n_rows, round_down=False)\n",
        "            output_images.insert(0, grid)\n",
        "\n",
        "    return output_images"
      ],
      "metadata": {
        "id": "WMtx0PgGSaE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text 2 Image\n",
        "\n",
        "512x512 works well, 640x640 might crash colab"
      ],
      "metadata": {
        "id": "thSWMltDLYNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\\uD83C\\uDF08\\uD83E\\uDD16\\uD83E\\uDD16, octane 3D render\" #@param {type:\"string\"}\n",
        "sampler = 'k-diffusion' #@param [\"k-diffusion\", \"PLMS\", \"DDIM\"] {allow-input: false}\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7 #@param {type:'integer'}\n",
        "n_steps = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "\n",
        "ddim_eta = 0.0\n",
        "n_iter = 1\n",
        "n_samples = 4 #@param {type:'integer'}\n",
        "grid = True #@param {type:\"boolean\"}\n",
        "n_rows = 2 #@param {type:'integer'}\n",
        "\n",
        "seed = 51234 #@param {type:'integer'}\n",
        "\n",
        "images = dream(prompt, n_steps, sampler, n_rows, ddim_eta, n_iter, n_samples, cfg_scale, seed, height, width, True, not(grid))\n"
      ],
      "metadata": {
        "id": "g0Sy0dk0Mkkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {cfg_scale}, steps = {n_steps}, sampler = {sampler}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "e5i1NKmCU8z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Clear GPU if OOM (Run from set up again)"
      ],
      "metadata": {
        "id": "F73YlrzHQxNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "akuObYB5Mv8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual outputs"
      ],
      "metadata": {
        "id": "UeAcuHx-Em0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "VwiRZr9xMxd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "gaR2KDdoMy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "7-YR0dsdM0JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "cusiIi8SM1Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image 2 Image"
      ],
      "metadata": {
        "id": "NwsSUAbMLe04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a girl in a blue dress, wearing a white nurse cap, fair skin, long dark hair. At night, starry sky, bright light from the big yellowish moon. The girt is reaching out to the moon with her arms. Passionate, sad, depression, longing. Beautiful landscape background.Pastel painting by Josan Gonzalez, thomas kinkade, trending on artstation, cgsociety, deviantart, f/22, 35mm, graphic novel style, bleak, dark, ominous, threatening, tumultuous, Post-apocalyptic, Memphis Group, 1980s, Extreme long shot, Bokeh, 11pm, moon light, Kodachrome, cinestill 800t\" #@param {type:\"string\"}\n",
        "init_image_path = \"/content/stable-diffusion/Source/bad_0300X.png\" #@param {type: 'string'}\n",
        "resize_mode = \"Just resize\" #@param [\"Just resize\", \"Crop and resize\", \"Resize and fill\"] {allow-input: false}\n",
        "\n",
        "init_image = Image.open(init_image_path)\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7 #@param {type:'integer'}\n",
        "n_steps = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "strength = 0.99 #@param {type: \"slider\", min:0.00, max:1.00, step:0.01}\n",
        "ddim_eta = 0.0\n",
        "n_iter = 1\n",
        "n_samples = 4 #@param {type:'integer'}\n",
        "grid = True #@param {type:\"boolean\"}\n",
        "n_rows = 2 #@param {type:'integer'}\n",
        "\n",
        "seed = 2043630622222 #@param {type:'integer'}\n",
        "\n",
        "mode = 0\n",
        "if resize_mode == \"Crop and resize\":\n",
        "  mode = 1\n",
        "if resize_mode == \"Resize and fill\":\n",
        "  mode = 2\n",
        "\n",
        "\n",
        "images = translation(prompt, init_image, mode, n_steps, n_rows, ddim_eta, n_iter, n_samples, cfg_scale, strength, seed, height, width, True, not(grid))\n"
      ],
      "metadata": {
        "id": "SHOh80nzT4jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Caption\n",
        "index = 0 #@param {type:\"integer\"}\n",
        "info = f\"seed = {seed}, cfg = {cfg_scale}, steps = {n_steps}, strength = {strength}\"\n",
        "captioned_image = caption(images[index], prompt, info)\n",
        "captioned_image"
      ],
      "metadata": {
        "id": "AfXX5n1TmBEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9) # Clear GPU if OOM or funny errors (Run from set up again)"
      ],
      "metadata": {
        "id": "mcDhMNZGVoyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "LLKFosZiVpqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Individual results"
      ],
      "metadata": {
        "id": "7fplwcZ_EZaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images[1]"
      ],
      "metadata": {
        "id": "sEG1FyrGVpgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[2]"
      ],
      "metadata": {
        "id": "2e1NLC_BVpZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[3]"
      ],
      "metadata": {
        "id": "1b6Qdz8BVpRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[4]"
      ],
      "metadata": {
        "id": "xytXKAcmVpJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image 2 Image (Sequence)"
      ],
      "metadata": {
        "id": "4D_g45M-F9e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running I2I Sequence"
      ],
      "metadata": {
        "id": "8lQVGJJFGGcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from IPython.display import clear_output \n",
        "import numpy as np\n",
        "import timeit\n",
        "#@title Image sequence\n",
        "#@markdown ##### Enter text prompt:\n",
        "prompt = \"man standing in front of a painting of a mountain in the distance, wide angle, Van Gogh, futuristic, surreal, holy\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ##### Set sequence duration (/Source/prefix_XXXXX.png):\n",
        "start_frame = \"00024\" #@param {type:\"string\"}\n",
        "end_frame = \"00037\" #@param {type:\"string\"}\n",
        "prefix = \"take\" #@param {type:\"string\"}\n",
        "current_frame = start_frame\n",
        "#@markdown ---\n",
        "#@markdown ##### SD settings:\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "\n",
        "cfg_scale = 7 #@param {type:'integer'}\n",
        "n_steps = 50 #@param {type:\"slider\", min:1, max:150, step:1}\n",
        "\n",
        "strength = 0.99 #@param {type: \"slider\", min:0.00, max:1.00, step:0.01}\n",
        "ddim_eta = 0.0\n",
        "n_iter = 1\n",
        "n_samples = 1\n",
        "grid = False\n",
        "n_rows = 1\n",
        "\n",
        "seed = 2043630622222 #@param {type:'integer'}\n",
        "\n",
        "timeStart = timeit.default_timer()\n",
        "for i in range(int(start_frame), int(end_frame) + 1):\n",
        "  current_frame = \"{:05d}\".format(i)\n",
        "  print(\"PROCESSING \" + current_frame)\n",
        "\n",
        "  init_image_path = f\"/content/stable-diffusion/Source/{prefix}_{current_frame}\"\n",
        "  init_image = Image.open(init_image_path)\n",
        "\n",
        "  \n",
        "  images[0].save(f\"/content/stable-diffusion/Output/{prefix}_{current_frame}.png\")\n",
        "  images[0]\n",
        "  images = translation(prompt, init_image, n_steps, n_rows, ddim_eta, n_iter, n_samples, cfg_scale, strength, seed, height, width, True, True)\n",
        "  clear_output()\n",
        "timeStop = timeit.default_timer()\n",
        "\n",
        "print(\"{} frames done in {} seconds.\\nPrompt: {}\\nStrength: {}\".format(int(end_frame) - int(start_frame), timeStop - timeStart, prompt, strength))"
      ],
      "metadata": {
        "id": "-LUoUqavGM2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Export video"
      ],
      "metadata": {
        "id": "mtsnExYRGKBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Export video\n",
        "from google.colab import files\n",
        "file_name = \"out.mp4\" #@param {type: \"string\"}\n",
        "always_overwrite = True #@param {type:\"boolean\"}\n",
        "framerate = 30 #@param (type:number)\n",
        "if always_overwrite:\n",
        "  replace = 'y'\n",
        "else:\n",
        "  replace = 'n'\n",
        "!yes {replace} | ffmpeg -framerate {framerate} -start_number {int(start_frame)} -i /content/stable-diffusion/Output/{prefix}_%05d.png -c:v libx264 -crf 0 {file_name}\n",
        "files.download('/content/stable-diffusion/{}.mp4'.format(file_name))"
      ],
      "metadata": {
        "id": "7rK2voJIHOt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving outputs"
      ],
      "metadata": {
        "id": "uZ13WzesoXdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/stable-diffusion/Output/\"\n",
        "name = \"out\" #@param {type:\"string\"}\n",
        "\n",
        "save_all = True #@param {type:\"boolean\"}\n",
        "\n",
        "if save_all:\n",
        "  k = 0\n",
        "  for i in images:\n",
        "    i.save(f'{path}{name}_{k}.png')\n",
        "    k += 1\n",
        "else:\n",
        "  index = 1 #@param {type:\"integer\"}\n",
        "  images[index].save(f'{path}{name}_{index}.png')"
      ],
      "metadata": {
        "id": "W_xIkXoMB9iQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jwZ0GT0eObBW",
        "XvHTXI7KOnu4",
        "yaHfjOHyJgdC",
        "N-ObhCcRHhFv",
        "Oudkt8AEKw05",
        "OmQfJWm8QGoA",
        "-7syo80FPa4T",
        "ihQD4BD1P1Vr",
        "P95iTXcZQbnk",
        "c6jepvv8Q-1a",
        "IFdeC2LZRBm-",
        "7oPo2rboRWfH",
        "NMj_JbQ9Rrxq",
        "5kQ2bDBrPe6Q",
        "TTdaBTHeCpUr",
        "livJxlGVCwCl",
        "eJsqY6L3C12V",
        "YbgZNNTxLOEo",
        "na17SiCBMY9Y",
        "UeAcuHx-Em0U",
        "4D_g45M-F9e2",
        "8lQVGJJFGGcK",
        "mtsnExYRGKBO",
        "uZ13WzesoXdq"
      ],
      "name": "Neo Hidamari Diffusion.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}